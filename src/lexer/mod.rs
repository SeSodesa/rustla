/// This is the `lexer` module of ruSTLa

pub mod token;
mod tests;

use std::fmt;
use crate::lexer::token::{Token, TokenType};

#[derive(PartialEq)]
pub struct Lexer {
  source: String,
  tokens: Vec<Token>,
  lexeme_start: usize,
  lookahead: usize,
  row: usize,
  col: usize,
}

impl fmt::Debug for Lexer {
  fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
      f.debug_struct("Lexer")
        .field("source", &self.source)
        .field("id", &self.tokens)
        .finish()
  }
}

/// Lexer type methods
impl Lexer {

  /// ### Lexer constructor
  pub fn new(source: String) -> Lexer {
    Lexer {
      source: source,
      tokens: Vec::new(),
      lexeme_start: 0,
      lookahead: 0,
      row:0,
      col: 0,
    }
  }

  /// ### scan_tokens
  /// Pushes the tokens generated by
  /// `scan_token` to `tokens`
  fn scan_tokens(&mut self) {

  }

  /// ### scan_token
  /// Reads the next lexeme and produces
  /// a token mathcing it. This is the
  /// core of the lexer itself.
  fn scan_token(&mut self) {

  }

  /// ### advance
  /// Reads the next character
  /// (unicode scalar, not grapheme cluster!)
  /// in the source.
  fn advance(&mut self) {

  }


  /// ### add_token
  /// Pushes a token from the lexeme between
  /// `lexeme_start` and `lookahead`
  fn add_token () {

  }

  /// ### is_at_eof
  /// A function that checks whether all
  /// of the characters in the current file
  /// have been consumed.
  pub fn is_at_eof(&self) -> bool {
    self.lookahead >= self.source.chars().count()
  }

}
